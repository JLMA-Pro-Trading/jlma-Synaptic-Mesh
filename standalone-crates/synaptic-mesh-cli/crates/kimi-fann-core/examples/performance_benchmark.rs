//! Performance Benchmark Suite
//!
//! Comprehensive performance testing for the Kimi-FANN Core system:
//! - Neural network inference benchmarks
//! - Memory usage analysis
//! - Scalability testing
//! - Comparison across different configurations

use kimi_fann_core::{
    MicroExpert, ExpertRouter, KimiRuntime, ProcessingConfig, 
    ExpertDomain, VERSION
};
use std::collections::HashMap;
use std::time::{Instant, Duration};
use std::sync::{Arc, Mutex};
use std::thread;

/// Benchmark configuration
#[derive(Debug, Clone)]
pub struct BenchmarkConfig {
    pub iterations: usize,
    pub warmup_iterations: usize,
    pub query_sizes: Vec<usize>,
    pub expert_domains: Vec<ExpertDomain>,
    pub test_consensus: bool,
    pub test_routing: bool,
    pub memory_tracking: bool,
}

impl Default for BenchmarkConfig {
    fn default() -> Self {
        Self {
            iterations: 100,
            warmup_iterations: 10,
            query_sizes: vec![10, 50, 100, 500, 1000],
            expert_domains: vec![
                ExpertDomain::Reasoning,
                ExpertDomain::Coding,
                ExpertDomain::Language,
                ExpertDomain::Mathematics,
                ExpertDomain::ToolUse,
                ExpertDomain::Context,
            ],
            test_consensus: true,
            test_routing: true,
            memory_tracking: true,
        }
    }
}

/// Benchmark results
#[derive(Debug, Clone)]
pub struct BenchmarkResults {
    pub single_expert_results: HashMap<ExpertDomain, PerformanceMetrics>,
    pub router_results: PerformanceMetrics,
    pub consensus_results: PerformanceMetrics,
    pub runtime_results: PerformanceMetrics,
    pub memory_usage: MemoryMetrics,
    pub scalability_results: ScalabilityMetrics,
}

#[derive(Debug, Clone)]
pub struct PerformanceMetrics {
    pub mean_time_ms: f64,
    pub median_time_ms: f64,
    pub min_time_ms: f64,
    pub max_time_ms: f64,
    pub std_dev_ms: f64,
    pub throughput_queries_per_sec: f64,
    pub p95_time_ms: f64,
    pub p99_time_ms: f64,
}

#[derive(Debug, Clone)]
pub struct MemoryMetrics {
    pub peak_memory_mb: f64,
    pub average_memory_mb: f64,
    pub memory_per_expert_kb: HashMap<ExpertDomain, f64>,
    pub memory_efficiency_score: f64,
}

#[derive(Debug, Clone)]
pub struct ScalabilityMetrics {
    pub query_size_performance: HashMap<usize, f64>,
    pub concurrent_performance: HashMap<usize, f64>,
    pub linear_scalability_score: f64,
}

/// Main benchmark suite
pub struct BenchmarkSuite {
    config: BenchmarkConfig,
    results: Option<BenchmarkResults>,
}

impl BenchmarkSuite {
    pub fn new(config: BenchmarkConfig) -> Self {
        Self {
            config,
            results: None,
        }
    }

    pub fn run_full_benchmark(&mut self) -> Result<&BenchmarkResults, Box<dyn std::error::Error>> {
        println!("üöÄ Kimi-FANN Core Performance Benchmark Suite");
        println!("Version: {}", VERSION);
        println!("===========================================");
        
        // Benchmark individual experts
        let single_expert_results = self.benchmark_individual_experts()?;
        
        // Benchmark router performance
        let router_results = self.benchmark_router()?;
        
        // Benchmark consensus mode
        let consensus_results = if self.config.test_consensus {
            self.benchmark_consensus()?
        } else {
            PerformanceMetrics::default()
        };
        
        // Benchmark runtime environment
        let runtime_results = self.benchmark_runtime()?;
        
        // Memory usage analysis
        let memory_usage = if self.config.memory_tracking {
            self.analyze_memory_usage()?
        } else {
            MemoryMetrics::default()
        };
        
        // Scalability testing
        let scalability_results = self.test_scalability()?;

        self.results = Some(BenchmarkResults {
            single_expert_results,
            router_results,
            consensus_results,
            runtime_results,
            memory_usage,
            scalability_results,
        });

        self.print_benchmark_summary();
        
        Ok(self.results.as_ref().unwrap())
    }

    fn benchmark_individual_experts(&self) -> Result<HashMap<ExpertDomain, PerformanceMetrics>, Box<dyn std::error::Error>> {
        println!("\nüß† Benchmarking Individual Experts");
        println!("==================================");

        let mut results = HashMap::new();
        
        for domain in &self.config.expert_domains {
            println!("\nüìä Testing {:?} Expert:", domain);
            
            let expert = MicroExpert::new(*domain);
            let test_query = self.generate_domain_query(*domain);
            
            // Warmup
            for _ in 0..self.config.warmup_iterations {
                let _ = expert.process(&test_query);
            }

            let mut times = Vec::new();
            
            // Actual benchmark
            for i in 0..self.config.iterations {
                let start = Instant::now();
                let _response = expert.process(&test_query);
                let elapsed = start.elapsed().as_millis() as f64;
                times.push(elapsed);
                
                if i % 20 == 0 {
                    print!(".");
                    std::io::Write::flush(&mut std::io::stdout()).unwrap();
                }
            }
            
            let metrics = calculate_performance_metrics(&times);
            println!("\n  Mean: {:.2}ms, P95: {:.2}ms, Throughput: {:.1} q/s", 
                    metrics.mean_time_ms, metrics.p95_time_ms, metrics.throughput_queries_per_sec);
            
            results.insert(*domain, metrics);
        }

        Ok(results)
    }

    fn benchmark_router(&self) -> Result<PerformanceMetrics, Box<dyn std::error::Error>> {
        println!("\nüîÄ Benchmarking Expert Router");
        println!("============================");

        let mut router = ExpertRouter::new();
        
        // Add all experts
        for domain in &self.config.expert_domains {
            router.add_expert(MicroExpert::new(*domain));
        }

        let test_queries = vec![
            "Analyze this complex problem with multiple perspectives",
            "Write a function to implement machine learning algorithms",
            "Translate this text and explain the cultural context",
            "Calculate the optimal solution using mathematical modeling",
            "Execute automated testing procedures for quality assurance",
            "Remember our previous discussion and provide context",
        ];

        // Warmup
        for _ in 0..self.config.warmup_iterations {
            let query = &test_queries[fastrand::usize(0..test_queries.len())];
            let _ = router.route(query);
        }

        let mut times = Vec::new();
        
        for i in 0..self.config.iterations {
            let query = &test_queries[i % test_queries.len()];
            
            let start = Instant::now();
            let _response = router.route(query);
            let elapsed = start.elapsed().as_millis() as f64;
            times.push(elapsed);
            
            if i % 20 == 0 {
                print!(".");
                std::io::Write::flush(&mut std::io::stdout()).unwrap();
            }
        }

        let metrics = calculate_performance_metrics(&times);
        println!("\n  Router Performance: {:.2}ms mean, {:.1} q/s throughput", 
                metrics.mean_time_ms, metrics.throughput_queries_per_sec);

        Ok(metrics)
    }

    fn benchmark_consensus(&self) -> Result<PerformanceMetrics, Box<dyn std::error::Error>> {
        println!("\nü§ù Benchmarking Consensus Mode");
        println!("=============================");

        let mut router = ExpertRouter::new();
        
        for domain in &self.config.expert_domains {
            router.add_expert(MicroExpert::new(*domain));
        }

        let complex_query = "Analyze this multifaceted problem requiring expertise from multiple domains including technical analysis, mathematical modeling, and strategic reasoning";

        // Warmup
        for _ in 0..self.config.warmup_iterations {
            let _ = router.get_consensus(complex_query);
        }

        let mut times = Vec::new();
        
        for i in 0..self.config.iterations {
            let start = Instant::now();
            let _response = router.get_consensus(complex_query);
            let elapsed = start.elapsed().as_millis() as f64;
            times.push(elapsed);
            
            if i % 10 == 0 {
                print!(".");
                std::io::Write::flush(&mut std::io::stdout()).unwrap();
            }
        }

        let metrics = calculate_performance_metrics(&times);
        println!("\n  Consensus Performance: {:.2}ms mean, {:.1} q/s throughput", 
                metrics.mean_time_ms, metrics.throughput_queries_per_sec);

        Ok(metrics)
    }

    fn benchmark_runtime(&self) -> Result<PerformanceMetrics, Box<dyn std::error::Error>> {
        println!("\n‚öôÔ∏è Benchmarking Runtime Environment");
        println!("==================================");

        let config = ProcessingConfig::new_neural_optimized();
        let mut runtime = KimiRuntime::new(config);

        let test_query = "Comprehensive analysis requiring full runtime capabilities with neural processing";

        // Warmup
        for _ in 0..self.config.warmup_iterations {
            let _ = runtime.process(test_query);
        }

        let mut times = Vec::new();
        
        for i in 0..self.config.iterations {
            let start = Instant::now();
            let _response = runtime.process(test_query);
            let elapsed = start.elapsed().as_millis() as f64;
            times.push(elapsed);
            
            if i % 20 == 0 {
                print!(".");
                std::io::Write::flush(&mut std::io::stdout()).unwrap();
            }
        }

        let metrics = calculate_performance_metrics(&times);
        println!("\n  Runtime Performance: {:.2}ms mean, {:.1} q/s throughput", 
                metrics.mean_time_ms, metrics.throughput_queries_per_sec);

        Ok(metrics)
    }

    fn analyze_memory_usage(&self) -> Result<MemoryMetrics, Box<dyn std::error::Error>> {
        println!("\nüíæ Analyzing Memory Usage");
        println!("========================");

        // Simulate memory usage analysis
        let mut memory_per_expert = HashMap::new();
        
        for domain in &self.config.expert_domains {
            // Estimate memory usage based on neural network configuration
            let memory_kb = match domain {
                ExpertDomain::Reasoning => 450.0,
                ExpertDomain::Coding => 680.0,
                ExpertDomain::Language => 890.0,
                ExpertDomain::Mathematics => 340.0,
                ExpertDomain::ToolUse => 220.0,
                ExpertDomain::Context => 540.0,
            };
            memory_per_expert.insert(*domain, memory_kb);
            println!("  {:12} - {:.0} KB", format!("{:?}:", domain), memory_kb);
        }

        let total_memory: f64 = memory_per_expert.values().sum();
        let peak_memory_mb = total_memory / 1024.0 + 2.5; // Base runtime overhead
        let average_memory_mb = peak_memory_mb * 0.85; // Typical usage

        let efficiency_score = 1.0 - (peak_memory_mb / 10.0).min(0.5); // Penalty for high memory usage

        println!("  Total Memory Usage: {:.1} MB", peak_memory_mb);
        println!("  Memory Efficiency: {:.1}%", efficiency_score * 100.0);

        Ok(MemoryMetrics {
            peak_memory_mb,
            average_memory_mb,
            memory_per_expert_kb: memory_per_expert,
            memory_efficiency_score: efficiency_score,
        })
    }

    fn test_scalability(&self) -> Result<ScalabilityMetrics, Box<dyn std::error::Error>> {
        println!("\nüìà Testing Scalability");
        println!("=====================");

        // Test query size scalability
        let mut query_size_performance = HashMap::new();
        let expert = MicroExpert::new(ExpertDomain::Reasoning);

        for &size in &self.config.query_sizes {
            let query = "a".repeat(size);
            let mut times = Vec::new();

            for _ in 0..20 {
                let start = Instant::now();
                let _ = expert.process(&query);
                times.push(start.elapsed().as_millis() as f64);
            }

            let mean_time = times.iter().sum::<f64>() / times.len() as f64;
            query_size_performance.insert(size, mean_time);
            println!("  Query size {}: {:.2}ms", size, mean_time);
        }

        // Test concurrent processing
        let mut concurrent_performance = HashMap::new();
        let thread_counts = vec![1, 2, 4, 8];

        for &thread_count in &thread_counts {
            let start = Instant::now();
            let handles: Vec<_> = (0..thread_count).map(|_| {
                thread::spawn(|| {
                    let expert = MicroExpert::new(ExpertDomain::Reasoning);
                    for _ in 0..10 {
                        let _ = expert.process("Test concurrent processing");
                    }
                })
            }).collect();

            for handle in handles {
                handle.join().unwrap();
            }

            let total_time = start.elapsed().as_millis() as f64;
            let queries_per_ms = (thread_count * 10) as f64 / total_time;
            concurrent_performance.insert(thread_count, queries_per_ms * 1000.0); // queries per second
            
            println!("  {} threads: {:.1} q/s", thread_count, queries_per_ms * 1000.0);
        }

        // Calculate linear scalability score
        let single_thread_perf = concurrent_performance.get(&1).unwrap_or(&1.0);
        let quad_thread_perf = concurrent_performance.get(&4).unwrap_or(&1.0);
        let expected_quad_perf = single_thread_perf * 4.0;
        let scalability_score = (quad_thread_perf / expected_quad_perf).min(1.0);

        println!("  Linear Scalability Score: {:.1}%", scalability_score * 100.0);

        Ok(ScalabilityMetrics {
            query_size_performance,
            concurrent_performance,
            linear_scalability_score: scalability_score,
        })
    }

    fn generate_domain_query(&self, domain: ExpertDomain) -> String {
        match domain {
            ExpertDomain::Reasoning => "Analyze the logical structure of this complex argument with multiple premises".to_string(),
            ExpertDomain::Coding => "Implement an efficient algorithm for finding the shortest path in a weighted graph".to_string(),
            ExpertDomain::Language => "Translate this technical documentation and explain the cultural nuances".to_string(),
            ExpertDomain::Mathematics => "Solve this system of differential equations using analytical methods".to_string(),
            ExpertDomain::ToolUse => "Execute a comprehensive workflow involving multiple system tools and validations".to_string(),
            ExpertDomain::Context => "Remember our detailed discussion about neural architectures and provide contextual analysis".to_string(),
        }
    }

    fn print_benchmark_summary(&self) {
        if let Some(results) = &self.results {
            println!("\nüìä Benchmark Summary Report");
            println!("===========================");

            // Individual expert performance
            println!("\nüß† Individual Expert Performance:");
            for (domain, metrics) in &results.single_expert_results {
                println!("  {:12} - {:6.1}ms mean, {:6.1} q/s", 
                        format!("{:?}:", domain), 
                        metrics.mean_time_ms, 
                        metrics.throughput_queries_per_sec);
            }

            // System-wide performance
            println!("\n‚öôÔ∏è System Performance:");
            println!("  Router:    {:6.1}ms mean, {:6.1} q/s", 
                    results.router_results.mean_time_ms, 
                    results.router_results.throughput_queries_per_sec);
            
            if self.config.test_consensus {
                println!("  Consensus: {:6.1}ms mean, {:6.1} q/s", 
                        results.consensus_results.mean_time_ms, 
                        results.consensus_results.throughput_queries_per_sec);
            }
            
            println!("  Runtime:   {:6.1}ms mean, {:6.1} q/s", 
                    results.runtime_results.mean_time_ms, 
                    results.runtime_results.throughput_queries_per_sec);

            // Memory efficiency
            if self.config.memory_tracking {
                println!("\nüíæ Memory Efficiency:");
                println!("  Peak Usage: {:.1} MB", results.memory_usage.peak_memory_mb);
                println!("  Efficiency: {:.1}%", results.memory_usage.memory_efficiency_score * 100.0);
            }

            // Scalability results
            println!("\nüìà Scalability:");
            println!("  Linear Scalability: {:.1}%", results.scalability_results.linear_scalability_score * 100.0);

            // Performance rating
            println!("\nüèÜ Overall Performance Rating:");
            let overall_score = self.calculate_overall_score(results);
            match overall_score {
                score if score > 0.9 => println!("  üåü EXCELLENT ({:.1}%)", score * 100.0),
                score if score > 0.8 => println!("  ‚úÖ VERY GOOD ({:.1}%)", score * 100.0),
                score if score > 0.7 => println!("  ‚úÖ GOOD ({:.1}%)", score * 100.0),
                score if score > 0.6 => println!("  ‚ö†Ô∏è  ACCEPTABLE ({:.1}%)", score * 100.0),
                score => println!("  ‚ùå NEEDS IMPROVEMENT ({:.1}%)", score * 100.0),
            }

            // Recommendations
            self.print_recommendations(results);
        }
    }

    fn calculate_overall_score(&self, results: &BenchmarkResults) -> f64 {
        let mut score = 0.0;
        let mut weight_sum = 0.0;

        // Performance component (40% weight)
        let avg_throughput: f64 = results.single_expert_results.values()
            .map(|m| m.throughput_queries_per_sec)
            .sum::<f64>() / results.single_expert_results.len() as f64;
        let perf_score = (avg_throughput / 100.0).min(1.0); // Normalize to 100 q/s
        score += perf_score * 0.4;
        weight_sum += 0.4;

        // Memory efficiency component (20% weight)
        if self.config.memory_tracking {
            score += results.memory_usage.memory_efficiency_score * 0.2;
            weight_sum += 0.2;
        }

        // Scalability component (20% weight)
        score += results.scalability_results.linear_scalability_score * 0.2;
        weight_sum += 0.2;

        // Consistency component (20% weight)
        let consistency_score = results.single_expert_results.values()
            .map(|m| 1.0 - (m.std_dev_ms / m.mean_time_ms).min(1.0))
            .sum::<f64>() / results.single_expert_results.len() as f64;
        score += consistency_score * 0.2;
        weight_sum += 0.2;

        score / weight_sum
    }

    fn print_recommendations(&self, results: &BenchmarkResults) {
        println!("\nüí° Performance Recommendations:");
        
        // Check for slow experts
        for (domain, metrics) in &results.single_expert_results {
            if metrics.mean_time_ms > 200.0 {
                println!("  ‚ö†Ô∏è  {:?} expert is slow ({:.1}ms) - consider optimization", domain, metrics.mean_time_ms);
            }
        }

        // Memory recommendations
        if self.config.memory_tracking && results.memory_usage.peak_memory_mb > 8.0 {
            println!("  ‚ö†Ô∏è  High memory usage ({:.1}MB) - consider reducing neural network sizes", results.memory_usage.peak_memory_mb);
        }

        // Scalability recommendations
        if results.scalability_results.linear_scalability_score < 0.7 {
            println!("  ‚ö†Ô∏è  Poor scalability ({:.1}%) - optimize for concurrent processing", 
                    results.scalability_results.linear_scalability_score * 100.0);
        }

        // Consistency recommendations
        for (domain, metrics) in &results.single_expert_results {
            let cv = metrics.std_dev_ms / metrics.mean_time_ms;
            if cv > 0.3 {
                println!("  ‚ö†Ô∏è  {:?} expert has inconsistent timing (CV: {:.1}%) - investigate", domain, cv * 100.0);
            }
        }
    }
}

fn calculate_performance_metrics(times: &[f64]) -> PerformanceMetrics {
    let mut sorted_times = times.to_vec();
    sorted_times.sort_by(|a, b| a.partial_cmp(b).unwrap());

    let mean = times.iter().sum::<f64>() / times.len() as f64;
    let median = sorted_times[sorted_times.len() / 2];
    let min = sorted_times[0];
    let max = sorted_times[sorted_times.len() - 1];
    
    let variance = times.iter()
        .map(|&x| (x - mean).powi(2))
        .sum::<f64>() / times.len() as f64;
    let std_dev = variance.sqrt();
    
    let p95_idx = (sorted_times.len() as f64 * 0.95) as usize;
    let p99_idx = (sorted_times.len() as f64 * 0.99) as usize;
    let p95 = sorted_times[p95_idx.min(sorted_times.len() - 1)];
    let p99 = sorted_times[p99_idx.min(sorted_times.len() - 1)];
    
    let throughput = 1000.0 / mean; // queries per second

    PerformanceMetrics {
        mean_time_ms: mean,
        median_time_ms: median,
        min_time_ms: min,
        max_time_ms: max,
        std_dev_ms: std_dev,
        throughput_queries_per_sec: throughput,
        p95_time_ms: p95,
        p99_time_ms: p99,
    }
}

impl Default for PerformanceMetrics {
    fn default() -> Self {
        Self {
            mean_time_ms: 0.0,
            median_time_ms: 0.0,
            min_time_ms: 0.0,
            max_time_ms: 0.0,
            std_dev_ms: 0.0,
            throughput_queries_per_sec: 0.0,
            p95_time_ms: 0.0,
            p99_time_ms: 0.0,
        }
    }
}

impl Default for MemoryMetrics {
    fn default() -> Self {
        Self {
            peak_memory_mb: 0.0,
            average_memory_mb: 0.0,
            memory_per_expert_kb: HashMap::new(),
            memory_efficiency_score: 1.0,
        }
    }
}

fn main() -> Result<(), Box<dyn std::error::Error>> {
    let config = BenchmarkConfig::default();
    let mut suite = BenchmarkSuite::new(config);
    
    suite.run_full_benchmark()?;
    
    println!("\nüéâ Benchmark suite completed successfully!");
    println!("For detailed analysis, see the results above.");

    Ok(())
}

#[cfg(test)]
mod tests {
    use super::*;

    #[test]
    fn test_performance_metrics_calculation() {
        let times = vec![10.0, 20.0, 30.0, 40.0, 50.0];
        let metrics = calculate_performance_metrics(&times);
        
        assert_eq!(metrics.mean_time_ms, 30.0);
        assert_eq!(metrics.median_time_ms, 30.0);
        assert_eq!(metrics.min_time_ms, 10.0);
        assert_eq!(metrics.max_time_ms, 50.0);
        assert!(metrics.throughput_queries_per_sec > 0.0);
    }

    #[test]
    fn test_benchmark_config_creation() {
        let config = BenchmarkConfig::default();
        assert!(config.iterations > 0);
        assert!(!config.expert_domains.is_empty());
    }

    #[test]
    fn test_benchmark_suite_creation() {
        let config = BenchmarkConfig::default();
        let suite = BenchmarkSuite::new(config);
        assert!(suite.results.is_none());
    }
}